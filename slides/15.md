layout: true

# Introduction to Concurrency

---

--

<div class="d-flex justify-content-center">
  <div class="card font-monospace" style="width: 28rem; font-size: 24px;">
    <div class="card-body">
          <div>A programmer had a problem. He thought to himself: "I know, I'll solve it with threads!".</div>
          <hr>
          <div>has Now, problems. two he</div>
    </div>
  </div>
</div>

---


#### What are the differences between *parallel*, *concurrent*, *asynchronous* and *multithreaded* programming? A lexicon.

--

**Parallel** programming involves dividing a problem into sub-problems, solving them simultaneously on multiple processors or cores, and then combining the results.

--

**Concurrent** programming involves structuring a program to handle multiple tasks that can make progress independently. These tasks might not run simultaneously but are designed to run independently. [(see parallel-vs-concurrent)](/images/parallel-vs-concurrent.png)

--

**Asynchronous** programming involves executing tasks that can start, run, and complete in the background without blocking the main thread of execution.

--

**Multithreaded** programming involves creating multiple threads within a single process to perform tasks concurrently, with each thread running independently but *sharing the same memory space*.

---


#### Why would anybody do parallel, concurrent, asynchronous or multithreaded programming ?

--

- Performance Improvement <sup>*</sup>

--

- Responsiveness and User Experience

--

- Efficiency in Resource Utilization

--

- Handling Real-Time Data

--

- Framework and Library Constraints <sup>*</sup>

---


### Demo Time!

Performance Improvement - Parallel vs Sequential Sum

<small>
The [ParallelVsSequentialSum.java](https://github.com/athenaeum-brew/concurrent-examples/blob/main/ParallelVsSequentialSum.java) Java program compares the efficiency of computing the sum of elements in an array sequentially versus in parallel. 

It initializes an array with integers from 1 to 10,000,000 and measures the execution time of both approaches. 

The sequential computation iterates through the array to calculate the sum, while the parallel computation divides the array into smaller tasks for concurrent processing. 

By comparing the execution times, users can observe the potential speedup achieved through parallelization. 
</small>


---


### Performance Improvement

Things to Note with ParallelVsSequentialSum.java program:

<small>
1. *Importance of Warm-up Phase*: Before measuring the parallel execution time, the program includes a warm-up phase. During this phase, a parallel sum computation is performed to prepare the thread pool for subsequent tasks. This helps in achieving more consistent and accurate performance measurements.

1. *Context Switching Between Threads*: The SEQUENTIAL_THRESHOLD in the program determines the granularity of tasks each thread handles, potentially affecting the number of threads involved in the computation by influencing the size of data chunks processed concurrently.

1. *ANSI Escape Codes, Number Formatting*: The program utilizes ANSI escape codes to add color to the command line output and underscores for number formatting, enhancing readability. Both features are definitely unrelated to parallel processing, just nice to know.
</small>

---


### Performance Improvement

**Must watch:** Thinking In Parallel by Stuart Marks and Brian Goetz.

https://youtu.be/2nup6Oizpcw

![Thinking In Parallel by Stuart Marks and Brian Goetz](/images/ThinkingInParallelbyStuartMarksandBrianGoetz.jpg)

---


### Demo Time nÂ°2!

Framework and Library Constraints

**Even if someone prefers not to directly use or understand Java threads, there's no escaping their usage when working with frameworks that utilize them.**

Introducing [jconsole](https://openjdk.org/tools/svc/jconsole/): demo with [HelloJavaFXWorld.java](https://github.com/athenaeum-brew/javafx-examples/tree/main/javafx-example-1-helloworld) and [HelloWorld.java](/samples/01/HelloWorld.java) programs.


---

## Recap

--
<small>
**Lexicon**

- *Parallel*: Simultaneous execution on multiple processors/cores.
- *Concurrent*: Structuring tasks to make independent progress.
- *Asynchronous*: Tasks running in the background without blocking the main thread.
- *Multithreaded*: Multiple threads within a process sharing memory space.

--

**Benefits of Concurrency**
- Performance Improvement
- Enhanced Responsiveness, Handling Real-Time Data
- Efficient Resource Utilization
- Framework Constraints

--

**Costs of Concurrency**
- Startup and Context Switching
- Complexity

<small>
.footnote[[More hexaustive list of costs](15-1.html)]
</small>

</small>

---

--

**Race Conditions**: Occur when the outcome of a program depends on the timing of uncontrollable events.

![race](/images/race.png)

---

# Introduction to Concurrency - Challenges

**Deadlocks**: Arise when two or more threads are blocked indefinitely, waiting for each other to release resources.

![deadlock](/images/deadlock.jpg)

---


Strategies

- **Synchronization**: Use synchronization mechanisms like locks and semaphores to coordinate access to shared resources.
- **Exception Handling**: Implement robust error handling mechanisms to gracefully handle exceptions and errors in concurrent environments.
- **Logging**: Implement effective logging to track the behavior of concurrent processes, aiding in debugging and understanding program flow.

--

Best Practices

- **Avoid Shared Mutable State**: Minimize shared mutable state between threads to reduce the likelihood of race conditions.
- **Fail Fast**: Detect errors early and fail fast to prevent them from propagating and causing further issues.

---


Locks

- **Mutexes**: Provide mutual exclusion, allowing only one thread to acquire the lock at a time.
- **Reentrant Locks**: Allow the same thread to reacquire the lock it already holds.
- **Read/Write Locks**: Differentiate between read-only and write operations, allowing multiple threads to read concurrently but ensuring exclusive access for write operations.

--

Semaphores

- **Counting Semaphores**: Allow a fixed number of threads to access a resource concurrently.

--

Atomic Operations

- **Atomic Variables**: Provide thread-safe operations on variables without the need for explicit synchronization.
- **Compare-and-Swap (CAS)**: Perform an atomic compare-and-swap operation to ensure consistency in concurrent environments.

---


Immutable Data Structures

- **Immutable Objects**: Create objects that cannot be modified after construction, ensuring thread safety.
- **Copy-On-Write Collections**: Use collections that create a new copy of the underlying data structure on modification, allowing safe concurrent access.

--

Thread-Local Variables

- **ThreadLocal<T>**: Store separate instances of variables for each thread, avoiding concurrency issues by eliminating shared state.

--

Synchronization Primitives

- **Volatile Variables**: Ensure visibility of changes across threads without providing mutual exclusion.
- **Synchronized Methods and Blocks**: Use synchronized methods or blocks to ensure atomicity and thread safety.

---


## Recap 2

<small>

**Concurrency Challenges**

- Identifies *race conditions *and *deadlocks* as major issues,
- Suggests strategies like synchronization, robust exception handling, careful logging,
- and best practices like avoiding shared mutable state and failing fast.

**Concurrency Coding Techniques** 
- [Synchronization primitives](15-2.html) (e.g., locks, semaphores, atomic operations)
- Thread-safe data structures (immutable objects, copy-on-write collections, thread-local variables)

**Concurrency Tools** 
- Logging
- Monitoring (jconsole. jprofile, etc.)



</small>


