# Introduction to Concurrency

--

<hr>

### A programmer had a problem. He thought to himself: "I know, I'll solve it with threads!". 

--

<hr>

### has Now, problems. two he

---

# Introduction to Concurrency

#### What are the differences between *parallel*, *concurrent*, *asynchronous* and *multithreaded* programming? A lexicon.

--

**Parallel** programming involves dividing a problem into sub-problems, solving them simultaneously on multiple processors or cores, and then combining the results.

--

**Concurrent** programming involves structuring a program to handle multiple tasks that can make progress independently. These tasks might not run simultaneously but are designed to run independently. [(see parallel-vs-concurrent)](/images/parallel-vs-concurrent.png)

--

**Asynchronous** programming involves executing tasks that can start, run, and complete in the background without blocking the main thread of execution.

--

**Multithreaded** programming involves creating multiple threads within a single process to perform tasks concurrently, with each thread running independently but *sharing the same memory space*.

---

# Introduction to Concurrency

#### Why would anybody do parallel, concurrent, asynchronous or multithreaded programming ?

--

- Performance Improvement

--

- Responsiveness and User Experience

--

- Efficiency in Resource Utilization

--

- Handling Real-Time Data

--

- Framework and Library Constraints

---

# Introduction to Concurrency

### Performance Improvement

Demo: Parallel vs Sequential Sum

The [ParallelVsSequentialSum.java](https://github.com/athenaeum-brew/concurrent-examples/blob/main/ParallelVsSequentialSum.java) Java program compares the efficiency of computing the sum of elements in an array sequentially versus in parallel. 

It initializes an array with integers from 1 to 10,000,000 and measures the execution time of both approaches. 

The sequential computation iterates through the array to calculate the sum, while the parallel computation divides the array into smaller tasks for concurrent processing. 

By comparing the execution times, users can observe the potential speedup achieved through parallelization. 


---

# Introduction to Concurrency

### Performance Improvement

Things to Note with [ParallelVsSequentialSum.java](/samples/15/ParallelVsSequentialSum.java) program:

1. *Importance of Warm-up Phase*: Before measuring the parallel execution time, the program includes a warm-up phase. During this phase, a parallel sum computation is performed to prepare the thread pool for subsequent tasks. This helps in achieving more consistent and accurate performance measurements.

1. *Context Switching Between Threads*: The SEQUENTIAL_THRESHOLD in the program determines the granularity of tasks each thread handles, potentially affecting the number of threads involved in the computation by influencing the size of data chunks processed concurrently.

1. *ANSI Escape Codes, Number Formatting*: The program utilizes ANSI escape codes to add color to the command line output and underscores for number formatting, enhancing readability. Both features are definitely unrelated to parallel processing, just nice to know.

---

# Introduction to Concurrency

### Performance Improvement

**Must watch:** Thinking In Parallel by Stuart Marks and Brian Goetz.

https://youtu.be/2nup6Oizpcw

![Thinking In Parallel by Stuart Marks and Brian Goetz](/images/ThinkingInParallelbyStuartMarksandBrianGoetz.jpg)

---

# Introduction to Concurrency

### Framework and Library Constraints

**Even if someone prefers not to directly use or understand Java threads, there's no escaping their usage when working with frameworks that utilize them.**

Introducing [jconsole](https://openjdk.org/tools/svc/jconsole/): demo with [HelloJavaFXWorld.java](https://github.com/athenaeum-brew/javafx-examples/tree/main/javafx-example-1-helloworld) and [HelloWorld.java](/samples/01/HelloWorld.java) programs.


---

# Introduction to Concurrency - Challenges

--

- **Race Conditions**: Occur when the outcome of a program depends on the timing of uncontrollable events.

![race](/images/race.png)

---

# Introduction to Concurrency - Challenges

- **Deadlocks**: Arise when two or more threads are blocked indefinitely, waiting for each other to release resources.

![deadlock](/images/deadlock.jpg)

---

# Introduction to Concurrency

Strategies

- **Synchronization**: Use synchronization mechanisms like locks and semaphores to coordinate access to shared resources.
- **Exception Handling**: Implement robust error handling mechanisms to gracefully handle exceptions and errors in concurrent environments.

--

Best Practices

- **Avoid Shared Mutable State**: Minimize shared mutable state between threads to reduce the likelihood of race conditions.
- **Fail Fast**: Detect errors early and fail fast to prevent them from propagating and causing further issues.

---

# Introduction to Concurrency

Locks

- **Mutexes**: Provide mutual exclusion, allowing only one thread to acquire the lock at a time.
- **Reentrant Locks**: Allow the same thread to reacquire the lock it already holds.
- **Read/Write Locks**: Differentiate between read-only and write operations, allowing multiple threads to read concurrently but ensuring exclusive access for write operations.

--

Semaphores

- **Counting Semaphores**: Allow a fixed number of threads to access a resource concurrently.

--

Atomic Operations

- **Atomic Variables**: Provide thread-safe operations on variables without the need for explicit synchronization.
- **Compare-and-Swap (CAS)**: Perform an atomic compare-and-swap operation to ensure consistency in concurrent environments.

---

# Introduction to Concurrency

#### Immutable Data Structures

- **Immutable Objects**: Create objects that cannot be modified after construction, ensuring thread safety.
- **Copy-On-Write Collections**: Use collections that create a new copy of the underlying data structure on modification, allowing safe concurrent access.

--

#### Thread-Local Variables

- **ThreadLocal<T>**: Store separate instances of variables for each thread, avoiding concurrency issues by eliminating shared state.

--

#### Synchronization Primitives

- **Volatile Variables**: Ensure visibility of changes across threads without providing mutual exclusion.
- **Synchronized Methods and Blocks**: Use synchronized methods or blocks to ensure atomicity and thread safety.

---

# Introduction to Concurrency - Summary

- *Concurrency Concepts*: Explains the differences between parallel, concurrent, asynchronous, and multithreaded programming.
- *Reasons for Concurrency*: Lists benefits such as performance improvement, better responsiveness, efficient resource utilization, handling real-time data, and the necessity due to framework constraints.
- *Performance Demonstration*: Describes a Java program comparing sequential and parallel sum calculations.
- *Concurrency Challenges*: Identifies race conditions and deadlocks as major issues, and suggests strategies like synchronization, robust exception handling, and best practices like avoiding shared mutable state and failing fast.
- *Concurrency Tools and Techniques*: Details synchronization primitives (e.g., locks, semaphores, atomic operations), thread-safe data structures (immutable objects, copy-on-write collections), and thread-local variables for managing concurrent execution.


